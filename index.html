<!DOCTYPE HTML>
<html lang="en"><head>
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6J8CBRENW5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6J8CBRENW5');
</script>
  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Charig Yang</title>
  
  <meta name="author" content="Charig Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!--<link rel="icon" type="image/png" href="images/seal_icon.png">-->

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Charig Yang</name>
              </p>
              <p>I am a PhD student at the Visual Geometry Group (VGG) at University of Oxford, advised by <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a> and <a href="https://weidixie.github.io/">Weidi Xie</a>, where I work on computer vision, more specifically video understanding. 
              </p>
              <p>
                I am generously funded by <a href="https://epsrc.ukri.org/">EPSRC</a> and <a href="https://aims.robots.ox.ac.uk/">AIMS CDT</a>. I also spent half a year interning at <a href="https://about.meta.com/uk/realitylabs/">Meta Reality Labs</a>.
              </p>
              <p>
                I did my undergraduate in Engineering Science, also at Oxford. During which, I spent lovely summers at <a href="https://www.jreast.co.jp/e/">Japan Railways</a>, <a href="https://www.metaswitch.com/">Metaswitch</a> (now acquired by Microsoft), <a href="https://www3.truecorp.co.th/new/">True</a>, <a href="https://www.cpgroupglobal.com/">CP Group</a>, and <a href="https://www.eng.ox.ac.uk/">Oxfordâ€™s Engineering Department</a>.
              </p>
              <p>
                Prior to which, I was born and raised in the suburbs of Bangkok, Thailand.
              </p>

              <p style="text-align:center">
                <a href="mailto:charig@robots.ox.ac.uk">Email</a> &nbsp/&nbsp
                <a href="res/cv.pdf">CV</a> &nbsp/&nbsp
                <!--<a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp-->
                <a href="https://twitter.com/chaaarig">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/charigyang/">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=nsCr3i8AAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="res/pp.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research focuses on computer vision. 
                I am particularly interested in: video understanding, self-supervised learning, segmentation, and new applications.
              </p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                      <tr>
            <td style="padding:40px;width:80%;vertical-align:middle">
              <a href="">
                <papertitle>Learning from Time</papertitle>
              </a>
              <br>
              <strong>Charig Yang</strong>
              <br>
              <em>PhD Thesis</em>
              <br>
              <a href="res/thesis_abs.txt">abstract</a> / thesis (coming soon)
              <br>
              <p></p>
              <p>Submitted my thesis! This thesis explores novel methods in learning and using temporal signals in videos. I will be defending in April. My examiners will be <a href="https://chrirupp.github.io/">Christian Rupprecht</a> (Oxford) and <a href="https://billf.mit.edu/">Bill Freeman</a> (MIT).</p>
            </td>
            </tr>
            <tr>
            <td style="padding:40px;width:80%;vertical-align:middle">
              <a href="https://charigyang.github.io/order/">
                <papertitle>Made to Order: Discovering monotonic temporal changes via self-supervised video ordering</papertitle>
              </a>
              <br>
              <strong>Charig Yang</strong>, 
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
              <br>
              <em>ECCV, 2024</em> <font color="red"><strong>(Oral Presentation)</strong></font> 
              <br>
              <a href="https://charigyang.github.io/order/">project page</a> /
              <a href="https://arxiv.org/abs/2404.16828">arXiv</a>
              <p></p>
              <p>Changes happen all the time, but only some are consistent over time. Ordering shuffled sequences reveals the latter.</p>
            </td>
            </tr>
            <tr>
            <td style="padding:40px;width:80%;vertical-align:middle">
              <a href="https://www.robots.ox.ac.uk/~vgg/research/flowsam/">
                <papertitle>Moving Object Segmentation: All You Need Is SAM (and Flow)</papertitle>
              </a>
              <br>
              <a href="https://github.com/Jyxarthur">Junyu Xie</a>,
              <strong>Charig Yang</strong>, 
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
              <br>
              <em>ACCV, 2024</em> <font color="red"><strong>(Oral Presentation)</strong></font> 
              <br>
              <a href="https://www.robots.ox.ac.uk/~vgg/research/flowsam/">project page</a> /
              <a href="https://arxiv.org/abs/2404.12389">arXiv</a>
              <p></p>
              <p>SAM + Optical Flow = FlowSAM.</p>
            </td>
                      </tr>
            <tr>
            <td style="padding:40px;width:80%;vertical-align:middle">
              <a href="https://charigyang.github.io/abouttime/">
                <papertitle>It's About Time: Analog Clock Reading in the Wild</papertitle>
              </a>
              <br>
              <strong>Charig Yang</strong>, 
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://charigyang.github.io/abouttime/">project page</a> /
              <a href="https://arxiv.org/abs/2111.09162">arXiv</a> /
              <a href="https://x.com/giffmana/status/1461249563466022913">tweet (by Lucas Beyer)</a> /
              <a href="https://www.newscientist.com/article/2298773-ai-has-learned-to-read-the-time-on-an-analogue-clock/">new scientist article</a> 
              <p></p>
              <p>We solve a niche but fun problem of reading clocks (that 2025's VLMs still fails!). We circumvent manual supervision by exploiting the fact that time flows at a constant rate.</p>
            </td>


                      </tr>
            <tr>

            <td style="padding:40px;width:80%;vertical-align:middle">
              <a href="https://charigyang.github.io/motiongroup/">
                <papertitle>Self-supervised Video Object Segmentation by Motion Grouping</papertitle>
              </a>
              <br>
              <strong>Charig Yang</strong>,
              <a href="https://twitter.com/hala_lamdouar/">Hala Lamdouar</a>,
              <a href="https://erikalu.com/">Erika Lu</a>,
              <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>
              <br>
              Short: CVPR Workshop on Robust Video Scene Understanding, 2021 
              <font color="red"><strong>(Best Paper Award)</strong></font> <br>
              Full: <em>ICCV</em>, 2021
              <br>
              <a href="https://charigyang.github.io/motiongroup/">project page</a> /
              <a href="https://arxiv.org/abs/2104.07658">arXiv</a>
              <p></p>
              <p>Motion can be used to discover moving objects in general. We introduce a self-supervised segmentation method by grouping motion into layers using a transformer.</p>
            </td>
            </tr>
            <tr>
            <td style="padding:40px;width:80%;vertical-align:middle">
              <a href="https://www.robots.ox.ac.uk/~vgg/data/MoCA/">
                <papertitle>Betrayed by Motion: Camouflaged Object Discovery via Motion Segmentation</papertitle>
              </a>
              <br>
              <a href="https://twitter.com/hala_lamdouar/">Hala Lamdouar</a>,
              <strong>Charig Yang</strong>,
              <a href="https://weidixie.github.io/">Weidi Xie</a>,
              <a href="https://www.robots.ox.ac.uk/~az/">Andrew Zisserman</a>
              <br>
              <em>ACCV</em>, 2020
              <br>
              <a href="https://www.robots.ox.ac.uk/~vgg/data/MoCA/">project page</a> /
              <a href="https://arxiv.org/abs/2011.11630">arXiv</a>
              <p></p>
              <p>Camouflaged animals are hard to see, only until they move. We present a method of discovering camouflages using motion, and a large-scale video camouflage dataset.</p>
            </td></tr>





          
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
              
              <p>
                2022-23: A2 (second-year) Electronic and Information Engineering, B14 (third-year) Information Engineering Systems, C18 (fourth-year) Computer Vision and Robotics
              <br>
                2021-22: B14 (third-year) Information Engineering Systems
              <br>                
                2020-21: P2 (first-year) Electronic and Information Engineering, A1 (second-year) Mathematics, C19 (fourth-year) Machine Learning
              </p>
              <p>
                You can find my summary notes for all P and A modules, and some B modules <a href="res/notes.zip">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template gratefully stolen from <a href="https://jonbarron.info/">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
